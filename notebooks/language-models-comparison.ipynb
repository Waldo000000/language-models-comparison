{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## N-gram language modelling\n\nWe'll start with a simple n-gram model, and use it to generate a random sentence.\n\nFirst, we'll import the necessary libraries. You'll need the nltk library for natural language processing and the random library to generate random text.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import nltk\nimport os\n\nnltk_data_dir = nltk.data.path[0]\nnltk_corpus_dir = os.path.join(nltk_data_dir, 'corpora', 'gutenberg')",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": "Load some text data. You can use the nltk library to load a sample text dataset. For example, you can load the text of Jane Austen's \"Emma\" with the following code:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import os\nimport shutil\n\nif not os.path.exists(nltk_corpus_dir):\n    os.makedirs(nltk_corpus_dir)\n\nshutil.copy('../data/austen-emma.txt', nltk_corpus_dir)\n\nemma = nltk.corpus.gutenberg.words('austen-emma.txt')",
      "metadata": {
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": "Preprocess the data. You'll need to clean and preprocess the text before you can use it to train an n-gram model. You can do this by converting the words to lowercase, removing punctuation, and joining the words into a single string. Here's some code that does this:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "emma = [word.lower() for word in emma if word.isalpha()]\ntext = ' '.join(emma)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": "Split the data into training and test sets. You'll need to split the text into two parts: one for training the n-gram model and one for testing it. Here's some code that splits the text into 80% training data and 20% test data:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_size = int(len(emma) * 0.8)\ntrain_data, test_data = emma[:train_size], emma[train_size:]",
      "metadata": {
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Create an n-gram model. You can use the nltk library to create an n-gram model. The following code creates a bigram model (i.e., a model that predicts the next word based on the previous word), specifically a dictionary model that maps each pair of words in the training data to a list of possible next words:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "n = 2\nbigrams = nltk.ngrams(train_data, n)\nmodel = {}\nfor gram in bigrams:\n    prev_words = ' '.join(gram[:-1])\n    last_word = gram[-1]\n    if prev_words in model:\n        model[prev_words].append(last_word)\n    else:\n        model[prev_words] = [last_word]",
      "metadata": {
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "source": "Now you can generate text using the model. Now that you've trained the model, you can use it to generate new text. You can start with a random word from the training data and then use the model to predict the next word based on the previous word. Below is some code that generates a sentence of 10 words using the bigram model. It generates a sentence of 10 words by starting with a random word from the training data and then using the bigram model to predict the next word based on the previous word:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import random\n\nseed_word = random.choice(train_data)\nsentence = [seed_word]\nfor i in range(9):\n    prev_words = ' '.join(sentence[-n+1:])\n    if prev_words in model:\n        next_word = random.choice(model[prev_words])\n    else:\n        next_word = random.choice(train_data)\n    sentence.append(next_word)\nprint(' '.join(sentence))",
      "metadata": {
        "trusted": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "text": "consider from foreseeing it was ending with her new proposal\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}