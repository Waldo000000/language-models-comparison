{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram language modelling\n",
    "\n",
    "We'll start with a simple n-gram model, and use it to generate a random sentence.\n",
    "\n",
    "First, we'll import the necessary libraries. You'll need the nltk library for natural language processing and the random library to generate random text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk_data_dir = nltk.data.path[0]\n",
    "nltk_corpus_dir = os.path.join(nltk_data_dir, 'corpora', 'gutenberg')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Load some text data. You can use the nltk library to load a sample text dataset. For example, you can load the text of Jane Austen's \"Emma\" with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\waldo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Preprocess the data. You'll need to clean and preprocess the text before you can use it to train an n-gram model. You can do this by converting the words to lowercase, removing punctuation, and joining the words into a single string. Here's some code that does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma = [word.lower() for word in emma if word.isalpha()]\n",
    "text = ' '.join(emma)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Split the data into training and test sets. You'll need to split the text into two parts: one for training the n-gram model and one for testing it. Here's some code that splits the text into 80% training data and 20% test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(emma) * 0.8)\n",
    "train_data, test_data = emma[:train_size], emma[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an n-gram model. You can use the nltk library to create an n-gram model. The following code creates a bigram model (i.e., a model that predicts the next word based on the previous word), specifically a dictionary model that maps each pair of words in the training data to a list of possible next words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "bigrams = nltk.ngrams(train_data, n)\n",
    "model = {}\n",
    "for gram in bigrams:\n",
    "    prev_words = ' '.join(gram[:-1])\n",
    "    last_word = gram[-1]\n",
    "    if prev_words in model:\n",
    "        model[prev_words].append(last_word)\n",
    "    else:\n",
    "        model[prev_words] = [last_word]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now you can generate text using the model. Now that you've trained the model, you can use it to generate new text. You can start with a random word from the training data and then use the model to predict the next word based on the previous word. Below is some code that generates a sentence of 10 words using the bigram model. It generates a sentence of 10 words by starting with a random word from the training data and then using the bigram model to predict the next word based on the previous word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that he was to her youth and was invited required\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed_word = random.choice(train_data)\n",
    "sentence = [seed_word]\n",
    "for i in range(9):\n",
    "    prev_words = ' '.join(sentence[-n+1:])\n",
    "    if prev_words in model:\n",
    "        next_word = random.choice(model[prev_words])\n",
    "    else:\n",
    "        next_word = random.choice(train_data)\n",
    "    sentence.append(next_word)\n",
    "print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
