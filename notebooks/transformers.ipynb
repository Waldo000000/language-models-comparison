{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb6889a-5c51-46a2-bb22-d8c266113b06",
   "metadata": {},
   "source": [
    "## What is a transformer?\n",
    "\n",
    "First, we'll need to install the transformers package in your JupyterLab environment. This package provides a simple way to use pre-trained transformer-based models in NLP. You can install it using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8792c7-eae0-40bd-8d31-c61d98b94f75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650e5996-293e-4fa3-8333-1e8f5c25c387",
   "metadata": {},
   "source": [
    "You'll also need to install PyTorch or Tensorflow. See e.g. https://www.tensorflow.org/install/:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40288a5b-825e-4010-961a-8806298e9211",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35aa6002-241b-4ced-9af8-ed55efbbd685",
   "metadata": {},
   "source": [
    "Once you've installed the package, we can start by importing the pipeline module from the transformers package. This module provides a simple interface for using pre-trained models to perform various NLP tasks, such as text generation or question answering. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "397261be-a216-425d-93b8-bb3e86021b99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)\"tf_model.h5\";: 100%|█████████████████████████| 498M/498M [00:57<00:00, 8.71MB/s]\n",
      "C:\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\waldo\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Downloading (…)neration_config.json: 100%|██████████████████████| 124/124 [00:00<00:00, 117kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████████████| 1.04M/1.04M [00:05<00:00, 174kB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|████████████████████| 456k/456k [00:01<00:00, 252kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████████████| 1.36M/1.36M [00:03<00:00, 352kB/s]\n",
      "C:\\Python310\\lib\\site-packages\\transformers\\generation\\tf_utils.py:603: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown foxes that grow in the forests of Australia, a species in which all the oxygen is lost along the way, are known to have\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the text generation pipeline using the GPT-2 model\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Generate a sample text sequence\n",
    "text = generator('The quick brown fox', max_length=30, num_return_sequences=1)[0]['generated_text']\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce1dd5b-5f78-4b27-9459-ce69bef94825",
   "metadata": {},
   "source": [
    "The code above has generated a short text sequence using the GPT-2 model. The `pipeline` function takes two arguments: the task to perform (in this case, text generation), and the name of the model to use (in this case, `gpt2`). The generator object returned by the pipeline function can be used to generate text sequences by calling it with a prompt string and some other options (such as the maximum length of the generated sequence and the number of sequences to generate)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d953f6-54ef-49a0-bc61-98e93cb0e891",
   "metadata": {},
   "source": [
    "Now that we've seen an example of how to use a transformer-based model for text generation, let's dive deeper into how transformers work. At a high level, a transformer is a neural network architecture that can process sequences of data, such as text. The basic idea behind a transformer is to use self-attention to allow the model to \"pay attention\" to different parts of the input sequence to make predictions.\n",
    "\n",
    "Self-attention is a mechanism that allows the model to attend to different parts of the input sequence to make predictions. The basic idea behind self-attention is to compute a weighted sum of the input sequence, where the weights are determined by a learned attention mechanism. Here's an example of how self-attention might work for a sequence of three words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12327505-1e24-427d-8355-beeda6e63fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
